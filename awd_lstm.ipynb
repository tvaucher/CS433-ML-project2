{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD LSTM Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the full implementation of the AWD LSTM neural net architecture used as language model in the [ULMFIT](https://arxiv.org/pdf/1801.06146.pdf) paper. Most of the upcoming code is heavily based on the [fastai](https://docs.fast.ai) library and its deep learning course, which has already a full implementation of the ulmfit approach for NLP. However considering the complexity of the fastai code and its simplicity to use we figured it would helpful for readers to get a full bottom up implementation using pytorch as a baseline. Still we expect that if you read this notebook, you have a good knowledge and understanding of RNNs, language modeling (see paper) and pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the AWD LSTM architecture is of course the LSTM neural net. It is an improvement to the standart RNN way of dealing with sequential data (such as text). LSTM deals with the [vanishing/exploding gradient problem](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) that come up in simple RNNs using cell connection gates. For further intuition on 'why' most the upcoming implentations I recommend colah's blog post : [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell and equations](images/lstm.jpg)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM archutecture is composed of a repeated cell which is shown in the image above. Its inputs are :\n",
    "- **xt** which in our case is the embedding vector of the nth word of a batch of sentences\n",
    "- **ht-1** the output of the last cell just like in RNNs.\n",
    "- **ct-1** again output form last cell which is called the *cell state* used to prevent long-term dependencies problem.\n",
    "\n",
    "The $\\sigma$ reprenstents the [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied element-wise to its input. Both x and + connections are elemnt-wise multiplication and addition respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement it using the pytorch nn.Module class. We use a two big matrix multiplication to compute x*U and and h*U instead of 4 for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.h_s = h_s\n",
    "        self.x_s = x_s\n",
    "        self.U = nn.Linear(x_s,4*h_s)\n",
    "        self.W = nn.Linear(h_s,4*h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        #inputs from last cell\n",
    "        h,c = state\n",
    "        \n",
    "        #computing itermedtiate gates\n",
    "        gates = (self.U(input) + self.W(h)).chunk(4, 1)\n",
    "        i_t,f_t,o_t = map(torch.sigmoid, gates[:3])\n",
    "        c_t = gates[3].tanh()\n",
    "        c = (f_t*c) + (i_t*c_t)\n",
    "        h = o_t * c.tanh()\n",
    "        \n",
    "        #outputting the usualt h output and the state to give to next cell if needed\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next building block of the LSTM is the LSTM layer wich consisit of appling the LSTM cell to each sequential input in a recurrent manner with each time forwarding its state to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM layer](images/LSTM3.png)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = LSTMCell(x_s, h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        # divide the input in the sequence dimension to get x_0, x_1, x_2, ...\n",
    "        inputs = input.unbind(1)\n",
    "        \n",
    "        #prepare to store the output of each cell\n",
    "        outputs = []\n",
    "        \n",
    "        #applying the cell recursively \n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.lstm_cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        \n",
    "        #return the stacked outputs\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the state for the first cell we simply use tensors with only zeroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before having fully implemented pytorch's LSTM module is stacking multiple LSTM layers one above each other as shown in the following diagram :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked RNN](images/RNN_Stacking.png)\n",
    "(picture from : https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/recurrent_neural_networks.html?q= )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='pink'>pink rectangles</font>  : The sequential inputs  \n",
    "- <font color='green'>green rectangles</font>  : An LSTM cell, each line is a layer so the cells on the same line are the same\n",
    "- <font color='blue'>blue rectangles</font> : The sequential outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous implementations, we created the initial state **h0** and **c0** outside of the model at the same time as we gave the input. This means that we could create h0 and c0 with the right sizes by simply comparing it to the input sizes. This time around, we will create the initial state to give to all the layers inside the model itself. As the dimensions of the state depend on the batch size of the input given, we need to create the initial states at the start of the forward pass when we are given the input (and thus we know the batch size) with the help of the **reset()** method. We also want to keep the last states from last batch if the batch size did not change.\n",
    "\n",
    "We aso have to be careful of the sives of the hidden layers inputs. For example, on the image above, the first layer takes as input the initial sequence and outputs a hidden sequence whereas the second and third layers take as input a hidden sequence and outputs a hidden sequence. Because of that we must have a different sizes for the first layer and the other layers. And of course the same goes for the initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullLSTM(nn.Module):\n",
    "    def __init__(self, x_s, h_s, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMLayer(x_s if i==0 else h_s, h_s ) for i in range(n_layers)])\n",
    "        self.bs = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        #get the batch size from the first dimension of the input \n",
    "        bs, sl, _ = input.size()\n",
    "        if self.bs != bs :\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "       \n",
    "        # now we have the initial states and we can go through all the layers recursively\n",
    "        for j in range(self.n_layers) :\n",
    "            layer = self.lstm_layers[j]\n",
    "            input, self.hidden[j] = layer(input, self.hidden[j])\n",
    "                \n",
    "        \n",
    "        #return the outputs\n",
    "        return input\n",
    "    \n",
    "    def reset(self) :\n",
    "        st = next(self.parameters()).new(self.bs, h_s).zero_()\n",
    "        self.hidden = [(st, st) for l in range(self.n_layers)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is pretty much the same as pytorch's nn.LSTM. The difference is that pytorch uses CuDNN to make the computations faster. We will now use pytorch's implementation instead of ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization : Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual regularization techniques used in feed-forward and convolutional neural nets such as dropout and batchnorm do not work well in RNNs. The AWD LSTM uses extensions of those to regularize its model. Correspondigns ections of the [paper](https://arxiv.org/pdf/1708.02182.pdf) will be provided for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational dropout \n",
    "Section 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in variational dropout is to use the **same** drop out mask to a squential input over the sequence dimension. In essence, if you have an input x with shape *(bs, seq_len, x_s)*, the dropout mask will be of shape *(bs, 1, x_s)* and will be applied to each slice of sequence.\n",
    "This dropout will be used on each output/input of the LSTM layers. Additionally we divide every activations that have not been set to 0 by the mask by 1-p (p: probability of dropout) to keep the average. We use [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) to be efficient in the element-wise computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDropout(nn.Module) :\n",
    "    def __init__(self, p=0.5) :\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x) :\n",
    "        #The dropout should only be used during training and not eval \n",
    "        if not self.training or self.p == 0.: return x\n",
    "        #the mask\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        #element-wise multiplication with broadcasting\n",
    "        return x*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.8941,  0.5911, -0.1017,  1.4399, -1.6575,  0.6746,  0.4393],\n",
       "          [-0.7288,  0.7585,  1.1424,  1.5135, -1.9283,  1.0334, -0.4931],\n",
       "          [-0.2847, -0.2492, -0.8360,  0.9637, -0.3897, -0.0111, -0.7577]],\n",
       " \n",
       "         [[ 0.7182,  1.2078,  0.0514,  0.2906,  1.9491,  0.9235,  0.3016],\n",
       "          [-0.2377,  1.4278, -0.4590, -0.0160,  0.9702, -0.3551,  0.3034],\n",
       "          [ 0.4043,  0.5498, -0.5338, -0.0617, -0.4097, -0.4234,  0.3979]],\n",
       " \n",
       "         [[-0.6652, -0.6661,  2.0000, -1.7812, -0.6374, -0.6694, -1.1852],\n",
       "          [-0.9589, -1.2132, -0.0476,  0.2971, -1.8044,  0.3118,  1.5059],\n",
       "          [-0.3349, -0.1490, -0.5113,  0.4419,  1.3988, -0.5077, -2.2353]]]),\n",
       " tensor([[[-0.8154,  0.4457, -0.7568, -2.3807,  1.9234, -0.0000,  1.5853],\n",
       "          [-1.0636, -0.1794, -1.0083,  2.3365, -0.6700, -0.0000, -0.3559],\n",
       "          [-1.4767,  3.2588, -0.9015,  1.4758, -1.0860, -0.0000, -1.5421]],\n",
       " \n",
       "         [[-0.2077,  0.5429, -0.3351, -1.9916,  2.1828,  0.7989,  0.8166],\n",
       "          [-0.3353, -1.3743, -0.2366,  0.5050,  0.1442,  0.2826,  0.9059],\n",
       "          [ 2.0066,  0.3356, -1.1485, -0.3782,  1.3648,  0.3348,  0.3471]],\n",
       " \n",
       "         [[-1.0524, -0.4202,  0.1573,  0.0000, -2.4432, -0.4741, -1.1690],\n",
       "          [ 3.2925,  0.7861, -0.7268,  0.0000, -0.1813, -0.1418, -1.2942],\n",
       "          [-0.6654, -1.0308, -0.0187, -0.0000, -3.2711,  3.1590, -0.2181]]]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNDropout(0.3)\n",
    "x = torch.randn(3,3,7)\n",
    "x, m(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dropped is consistent in the second dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding dropout \n",
    "Section 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding dropout we simply nulifiy entire rows of the word embedding matrix with probability p. Again broadcastiong is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0369, -4.0773, -1.0820, -2.0126, -1.0526,  0.7776, -2.6893],\n",
       "        [ 1.2100,  5.0159,  0.5262,  1.1307, -1.9710,  3.0681,  0.3349],\n",
       "        [ 1.2875, -5.1421, -0.1379, -2.6644,  1.0783, -1.3284,  2.4453],\n",
       "        [ 3.9419, -1.8886,  0.0901,  0.2247,  1.3759,  0.3034,  0.0748],\n",
       "        [ 0.7805, -2.3229,  0.7034,  1.4335,  0.6519,  0.2499,  2.6581],\n",
       "        [ 0.6230,  1.3893,  2.1869, -0.1947,  1.9668,  2.0854,  2.4333],\n",
       "        [-0.4102, -1.8052,  1.8866,  1.1053, -1.2866,  1.9148,  1.5755],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that entire rows have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight-dropout\n",
    "Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight dropout is a dropout applied to the weights inside the LSTM cells : U and W.\n",
    "\n",
    "In order to keep the speed of the LSTM layer, we simply replace the weight matrix of the LSTM by a masked version and keep the non-masked version. We can then simply apply the LSTM layer and it will use its new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the parameter in the nn.LSTM module containing the weights \n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            #\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0398,  0.6572],\n",
       "        [ 0.5677, -0.6067],\n",
       "        [ 0.1554, -0.3794],\n",
       "        [ 0.4172,  0.6862],\n",
       "        [-0.3063, -0.5804],\n",
       "        [-0.1082, -0.1653],\n",
       "        [ 0.6647, -0.3769],\n",
       "        [-0.4278, -0.3355]], requires_grad=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = WeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0663,  0.0000],\n",
       "        [ 0.0000, -1.0111],\n",
       "        [ 0.2590, -0.6323],\n",
       "        [ 0.6953,  0.0000],\n",
       "        [-0.0000, -0.9673],\n",
       "        [-0.1804, -0.2754],\n",
       "        [ 1.1079, -0.0000],\n",
       "        [-0.7129, -0.5591]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4,20,5)\n",
    "h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n",
    "x,h = dp_module(tst_input,h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dropout is applied to the weights during the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have everything ready to implement the entire AWSD LSTM model, the following code might look really complicated at first but it is in fact pretty much the same as our fullLSTM except we use the different kinds of dropout disscussed above. It also takes care of the word embeddings whereas our fullLSTM assumed it was already done so we need to take care of that. Another difference is that the last layer outputs a different size tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"Returns an iterator over module parameters.\n",
    "\n",
    "        This is typically passed to an optimizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_sz (int): number of words in the vocab\n",
    "            emb_sz (int): size of the word embedding vector\n",
    "            n_hid (int): size of the hidden vector \n",
    "            n_layers (int): number of layers in the LSTM\n",
    "            pad_token (int): id of the pad_idx for the embedding matrix\n",
    "            hidden_p (float): dropout probability for variational dropout on hidden activations\n",
    "            input_p (float): dropout probability for variational dropout on input activations\n",
    "            embed_p (float):dropout probability for embedding dropout \n",
    "            weight_p (float):dropout probability for weight dropout\n",
    "\n",
    "        \"\"\"\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        print(raw_output.shape)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a decoder which takes the output of our AWD_LSTM and transform it into the prediction of the wrord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both of them using a sequential module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
