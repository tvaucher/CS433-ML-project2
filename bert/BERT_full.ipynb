{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-433 2019 - Text Classification\n",
    "\n",
    "## Foreword\n",
    "This Notebook contains all the preprocessing/training/classification pipeline for the [text classification challenge](https://www.aicrowd.com/challenges/epfl-ml-text-classification-2019/leaderboards)\n",
    "\n",
    "It was run on a [Google Cloud Deep Learning VM](https://cloud.google.com/ai-platform/deep-learning-vm/docs) based on the [PyTorch image](https://cloud.google.com/ai-platform/deep-learning-vm/docs/pytorch_start_instance). The vm was configured with 2 vCPUs (13Go of RAM) and a Nvidia Tesla P100\n",
    "\n",
    "Thus the only missing package is [transformers](https://huggingface.co/transformers/index.html)\n",
    "\n",
    "### Disclaimer\n",
    "We don't recomand training the model without a beefy GPU (like an RTX 2080 Ti) @home, inference can be done on Google Colab pretty easily, but we provide a `run.py` for reference. See the [`README`](README.md) for details\n",
    "\n",
    "## 0. Setup\n",
    "**Google Cloud** for the entire proccess or **Google Colab** (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current GPU infos\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "If you are not using an online source. Assuming an active conda env, run:\n",
    "```shell\n",
    "conda install pytorch cudatoolkit=10.1 -c pytorch\n",
    "conda install pandas tqdm spacy numpy scikit-learn\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "from os import mkdir\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,\\\n",
    "                         AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "\n",
    "from preprocessing import tokenize, transform\n",
    "from helpers import BatchGenerator, get_device, set_seed, create_csv_submission\n",
    "from run import TestModel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "TOKENIZER = BertTokenizer\n",
    "MODEL = BertForSequenceClassification\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "LOG_EVERY = 3000\n",
    "LR = 1e-5\n",
    "MAX_GRAD_NORM = 1\n",
    "NUM_TRAINING_STEPS = 300_000\n",
    "NUM_WARMUP_STEPS = 30_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "The preprocessing is the same as our GloVe based models, we just drop the '<' and '>'\n",
    "\n",
    "**1.1 Preprocessing functions**\n",
    "From `preprocessing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Preprocess the train data**\n",
    "Methods adapted from our GloVe model\n",
    "1. Load data from file `.txt`\n",
    "2. Preprocess the data using `tokenize` and `transform`\n",
    "3. Save as `.tsv` for reusability\n",
    "\n",
    "*Training Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(positive_file, negative_file, out_file):\n",
    "    with open(negative_file, 'r', encoding='utf-8') as neg,\\\n",
    "            open(positive_file, 'r', encoding='utf-8') as pos,\\\n",
    "            open(out_file, 'w', encoding='utf-8') as out:\n",
    "        print('label\\ttweet', file=out)\n",
    "        for l in tqdm(neg, total=1250000, desc='Neg'):\n",
    "            print('0\\t' + transform(' '.join(tokenize(l))), file=out)\n",
    "        for l in tqdm(pos, total=1250000, desc='Pos'):\n",
    "            print('1\\t' + transform(' '.join(tokenize(l))), file=out)\n",
    "    \n",
    "preprocess_train('data/train_pos_full.txt',\n",
    "                 'data/train_neg_full.txt',\n",
    "                 'data/train_preprocessed.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Encoding procedure**\n",
    "1. Load preprocessed data into Pandas df\n",
    "2. Apply encoding from the tokenizer to transform text to list of tensors (Wordpiece indexes)\n",
    "3. (opt) Save as `pkl.gz` for reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TOKENIZER.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def encode_df(df, tokenizer, save=True, path='data/train_encoded_BERT.pkl.gz'):\n",
    "    tqdm.pandas()\n",
    "    df['tensor'] = df.tweet.progress_apply(lambda x: torch.tensor(tokenizer.encode(x),\n",
    "                                                                  dtype=torch.long))\n",
    "    df['length'] = df.tensor.progress_apply(len)\n",
    "    df = df[['label', 'tensor', 'length']].sort_values(by='length')\n",
    "    if save:\n",
    "        df[['label', 'tensor']].to_pickle(path, compression='gzip')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Encode the train set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_preprocessed.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = encode_df(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preprocess + encode the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test(path='data/test_data.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as test_file:\n",
    "        test_lines = [line.rstrip('\\n').split(',', 1) for line in test_file]\n",
    "        df = pd.DataFrame(test_lines, columns=['label', 'tweet'])\n",
    "        df.tweet = df.tweet.apply(lambda x: transform(' '.join(tokenize(x))))\n",
    "        return df\n",
    "\n",
    "test = encode_df(read_test(), tokenizer, save=True, path='data/test_encoded_BERT.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below to load the encoded df (if you have downloaded our encoded training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('data/train_encoded_BERT.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set up the seed for reproducability + select device (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split the data (90% train, 10% test) + generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, train_size=0.9, random_state=SEED, shuffle=False)\n",
    "train_batch = BatchGenerator(train, BATCH_SIZE, device)\n",
    "val_batch = BatchGenerator(val, BATCH_SIZE, device, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, batch, text):\n",
    "    ''' Make predicitons for a set (train/val), print and return the accuracy'''\n",
    "    val_pred = []\n",
    "    val_target = []\n",
    "    model.eval()\n",
    "    for seq, mask, labels in tqdm(batch):\n",
    "        pred = model(seq, attention_mask=mask)[0]\n",
    "        val_pred.append(pred.argmax(axis=1))\n",
    "        val_target.append(labels)\n",
    "    accuracy = (torch.cat(val_pred) == torch.cat(val_target)).float().mean().item()\n",
    "    print(text, accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, scheduler, train_batch, val_batch, log_every=None, n_epoch=3, path_prefix='model/checkpoint_BERT_'):\n",
    "    ''' \n",
    "    Fit the model using the optimizer and scheduler of n_epoch on the train_batch\n",
    "    Every log_every step, print the current train loss\n",
    "    At the end of each epoch, save the model under path_prefix and compute the val accuracy and loss over val_batch\n",
    "    '''\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_loss = 0\n",
    "        print('EPOCH', epoch)\n",
    "        model.train()\n",
    "        for i, (seq, mask, labels) in enumerate(tqdm(train_batch)):\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred = model(seq, attention_mask=mask, labels=labels)[:2]\n",
    "            sum_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if log_every and i % log_every == log_every-1:\n",
    "                print(i+1, sum_loss)\n",
    "                sum_loss = 0\n",
    "        \n",
    "        # Save current model\n",
    "        path = path_prefix + str(epoch)\n",
    "        try:\n",
    "            mkdir(path)\n",
    "        except:\n",
    "            pass\n",
    "        model.save_pretrained(path)\n",
    "        \n",
    "        # Compute accuracy at the end of the epoch\n",
    "        #compute_accuracy(model, train_batch, f'Train accuracy {epoch}')\n",
    "        compute_accuracy(model, val_batch, f'Validation accuracy {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create Model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL.from_pretrained(MODEL_NAME).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LR) \n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS,\n",
    "                                            num_training_steps=NUM_TRAINING_STEPS)\n",
    "\n",
    "fit(model, optimizer, scheduler, train_batch, val_batch, log_every=LOG_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this part only if you have downloaded our model + encoded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel(MODEL, 'model/checkpoint_BERT_1', 'data/test_encoded_BERT.pkl.gz', BATCH_SIZE, device)\n",
    "test_ids, test_pred = model.make_predictions()\n",
    "create_csv_submission(test_ids, test_pred, 'data/best_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
