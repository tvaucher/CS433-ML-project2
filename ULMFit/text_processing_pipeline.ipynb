{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "from functools import partial\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import spacy\n",
    "import re\n",
    "import collections\n",
    "from tqdm import tqdm_notebook\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the text processing that we will use to create the data used to train the models. (Most of the code is directly taken from fastai's deep learning course : [Deep Learning from the foundations](https://course.fast.ai/part2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to gradually build our data in order to get our data ready to use for training our models. The steps are :\n",
    "1. Load the data into an list of items (for text it can be a list containing chunks of text for example)\n",
    "2. Split the data into train and validation datasets\n",
    "3. For each of the datasets, process the elements (numericalize the text...) and label them (sentiment for seentiment analysis)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    \"\"\"\n",
    "    apllies (ordered) functions in funcs sqeuentially to x and return result\n",
    "    \"\"\"\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a list data structure : **ListContainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListContainer():\n",
    "    \"\"\"\n",
    "    At simple data structure for creating lists\n",
    "    \"\"\"\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __getitem__(self, idx):\n",
    "        try: return self.items[idx]\n",
    "        except TypeError:\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==len(self) # bool mask\n",
    "                return [o for m,o in zip(idx,self.items) if m]\n",
    "            return [self.items[i] for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ItemList** is a Listcontainer that can transform the elements before they are beeing accessed. Transforms are mainly used for data augmentation (in images ofr example) which we will not use for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemList(ListContainer):\n",
    "    \"\"\"\n",
    "    A listContainer containing items that can be tranformed before accessed \n",
    "    \"\"\"\n",
    "    def __init__(self, items, path='.', tfms=None):\n",
    "        super().__init__(items)\n",
    "        self.path,self.tfms = Path(path),tfms\n",
    "\n",
    "    def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n",
    "    \n",
    "    def new(self, items, cls=None):\n",
    "        if cls is None: cls=self.__class__\n",
    "        return cls(items, self.path, tfms=self.tfms)\n",
    "    \n",
    "    def  get(self, i): return i\n",
    "    def _get(self, i): return compose(self.get(i), self.tfms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        if isinstance(res,list): return [self._get(o) for o in res]\n",
    "        return self._get(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **SplitData** object will contain two ItemLists, one for the training data and the other for validation data. We can create one from an ItemList and a splitting function using the class method *split_by_func*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_func(items, f):\n",
    "    \"\"\"\n",
    "    Splits an list of items into two lists with the spliting func f\n",
    "    \"\"\"\n",
    "    mask = [f(o) for o in items]\n",
    "    # `None` values will be filtered out\n",
    "    f = [o for o,m in zip(items,mask) if m==False]\n",
    "    t = [o for o,m in zip(items,mask) if m==True ]\n",
    "    return f,t\n",
    "\n",
    "class SplitData():\n",
    "    \"\"\"\n",
    "    Contains a training and validation list of items\n",
    "    \"\"\"\n",
    "    def __init__(self, train, valid): self.train,self.valid = train,valid\n",
    "        \n",
    "    def __getattr__(self,k): return getattr(self.train,k)\n",
    "    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors\n",
    "    def __setstate__(self,data): self.__dict__.update(data) \n",
    "    \n",
    "    @classmethod\n",
    "    def split_by_func(cls, il, f):\n",
    "        lists = map(il.new, split_by_func(il.items, f))\n",
    "        return cls(*lists)\n",
    "\n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "    \"\"\"\n",
    "    Parent class for processors\n",
    "    \"\"\"\n",
    "    def process(self, items): return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabeledData** is kind of an equivalent to t he pytorch dataset, it contains the data x and the data target y and you can access it by index to get the tuple $(x_i, y_i)$. You can also add processes for x and y which we will use to preprocess (clean, tokenize and numericalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_by_func(ds, f, cls=ItemList): \n",
    "    \"\"\"\n",
    "    Returns a new ItemList containig the labels of evrey item in list ds using function f \n",
    "    \"\"\"\n",
    "    return cls([f(o) for o in ds.items], path=ds.path)\n",
    "\n",
    "\n",
    "class LabeledData():\n",
    "    \"\"\"\n",
    "    Contains label data that have been processed \n",
    "    \"\"\"\n",
    "    def process(self, il, proc): return il.new(compose(il.items, proc))\n",
    "\n",
    "    def __init__(self, x, y, proc_x=None, proc_y=None):\n",
    "        self.x, self.y = self.process(x, proc_x),self.process(y, proc_y)\n",
    "        self.proc_x, self.proc_y = proc_x,proc_y\n",
    "        \n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx): return self.x[idx],self.y[idx]\n",
    "    def __len__(self): return len(self.x)\n",
    "    \n",
    "    def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x)\n",
    "    def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y)\n",
    "    \n",
    "    def obj(self, items, idx, procs):\n",
    "        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)\n",
    "        item = items[idx]\n",
    "        for proc in reversed(listify(procs)):\n",
    "            item = proc.deproc1(item) if isint else proc.deprocess(item)\n",
    "        return item\n",
    "\n",
    "    @classmethod\n",
    "    def label_by_func(cls, il, f, proc_x=None, proc_y=None):\n",
    "        return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_by_func(sd, f, proc_x=None, proc_y=None):\n",
    "    \"\"\"\n",
    "    Transform splitted data sd into splitted labled data using splitter f and processes proc_x and proc_y \n",
    "    \"\"\"\n",
    "    train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)\n",
    "    return SplitData(train,valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need our ItemList for the tweets which take the list of all tweets contained in the df (or csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextList(ItemList) :\n",
    "    @classmethod\n",
    "    def from_df(cls, df, text_col):\n",
    "        texts = df[text_col]\n",
    "        texts = texts.values\n",
    "        return cls(texts)\n",
    "    @classmethod\n",
    "    def from_csv(cls,  path, text_col):\n",
    "        df = pd.read_csv(path)\n",
    "        return cls.from_df(df, text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TextList.from_csv('data/train_full_m_shuffled.csv', 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (2500000 items)\n",
       "['<number> words , <number> seconds , <number> moment : \" thank you god . \" rt this and be thankful not today , but everyday ! \\n'\n",
       " 'crime scene investigators : uncovering the truth ( blazers <neutralface> ine of duty describes crime scene investigators , in \\n'\n",
       " \"gosh this is it . the end of our baguio trip . i 'll miss this place , see you next year my beloved baguio city . \\n\"\n",
       " \"<hashtag> # teamfollowback kate middleton 's wedding day hairdresser reveals his secret assistant ( people magazine share \\n\"\n",
       " 'hope everyone has a good <number>/<number> . ill be at work being my little sober self <hashtag> # gettinthatmoney \\n'\n",
       " '<user> please follow me . <repeat> ( \\n' 'that te <smile> t i just got ( > _ \\n'\n",
       " '<user> follow me please justin i love you <heart> \\n'\n",
       " 'now hiring : front office administrative assistant at adecco ( westminster , co adecco has an immediate nee . <repeat> <url> <hashtag> # jobs \\n'\n",
       " \"<user> pretty please follow me cher ! i follow <user> and i 've been trying forever ! you 're my inspiration <hashtag> # cherlloydintheusa <number> \\n\"...]\n",
       "Path: ."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to split it into two datasets by giving it a random splitter which will, for every tweet, put it into the training set with probability $1-pctg$ or in the validation set with probability $pctg$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_splitter(item, pctg=0.2):\n",
    "    test = np.random.uniform(0, 1)\n",
    "    if test < pctg :\n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(tl, partial(random_splitter, pctg=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (2000253 items)\n",
       "['<number> words , <number> seconds , <number> moment : \" thank you god . \" rt this and be thankful not today , but everyday ! \\n', 'crime scene investigators : uncovering the truth ( blazers <neutralface> ine of duty describes crime scene investigators , in \\n', \"gosh this is it . the end of our baguio trip . i 'll miss this place , see you next year my beloved baguio city . \\n\", \"<hashtag> # teamfollowback kate middleton 's wedding day hairdresser reveals his secret assistant ( people magazine share \\n\", 'hope everyone has a good <number>/<number> . ill be at work being my little sober self <hashtag> # gettinthatmoney \\n', '<user> please follow me . <repeat> ( \\n', 'that te <smile> t i just got ( > _ \\n', '<user> follow me please justin i love you <heart> \\n', 'now hiring : front office administrative assistant at adecco ( westminster , co adecco has an immediate nee . <repeat> <url> <hashtag> # jobs \\n', 'danny glover ( black americans of achievement <url> \\n'...]\n",
       "Path: .\n",
       "Valid: TextList (499747 items)\n",
       "[\"<user> pretty please follow me cher ! i follow <user> and i 've been trying forever ! you 're my inspiration <hashtag> # cherlloydintheusa <number> \\n\", '<user> <user> thanx dear . <repeat> u welcome . <repeat> i hope u have a good one love \\n', \"i do n't even know why i try to hold back by tears while watching marley & me <hashtag> # getsmeeverytime \\n\", '<user> you look at you all fancy already ! <heart> your background . \\n', '<user> mh <elong> , i wonder how that shit turned out . <repeat> lols \\n', 'o <elong> no <elong> ( ( t . t <user> rt <user> linnarenfro colton is eliminated . <repeat> \\n', \"that photoshoot was so fun <smile> had a great time met some more nice people ca n't wait to come back to philippines ! \\n\", \"that 's fucked up . i 'm sure there 's other girls that would know how to treat you . <heart> rt <user> i did until she stood me up . twice . <repeat> \\n\", \"watching the help again . waiting on the kids to finish eating so i can clean the kitchen . <user> left for work , and i 'm bored . \\n\", 'p . s . <hashtag> # rip bert weedon , the original guitar hero \\n'...]\n",
       "Path: ."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process and tokenize the text. This will go as follows :\n",
    "- We first process the text non-tokenized with some preprocessing rules\n",
    "- We then tokenize the text to have a list of tokens (strings)\n",
    "- And finally we process the list of tokens with some postprocessing rules\n",
    "\n",
    "The TokenizeProcessor will apply the whole process to chunks of data in parallel in order to make it faster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "    \n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Postprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(func, arr, max_workers=4):\n",
    "    \"\"\"\n",
    "    Applies in parallel the func to the elements of arr\n",
    "    \"\"\"\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(tqdm_notebook(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]): return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): \n",
    "        self.chunksize,self.max_workers = chunksize,max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "\n",
    "    def proc_chunk(self, args):\n",
    "        i,chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, items): \n",
    "        toks = []\n",
    "        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "    \n",
    "    def proc1(self, item): return self.proc_chunk([item])[0]\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<number> words , <number> seconds , <number> moment : \" thank you god . \" rt this and be thankful not today , but everyday ! \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda89ddfbb0c4592bfa6e993f1b80c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xxbos • < • number • > • words • , • < • number • > • seconds • , • < • number • > • moment • : • \" • thank • you • god • . • \" • rt • this • and • be • thankful • not • today • , • but • everyday • ! • \\n • xxeos'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' • '.join(tp(tl[:1000])[0])[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep leanrnig model can obviously not take strings as input, so we need to map the tokens to an integer using again a processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): \n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o)\n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) \n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model data\n",
    "\n",
    "For the language model, the input/output of the model are not simple processed tweets and labels, it is a bit more complicated than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea of a language model is to learn how to \"speak\" the language and in order to do that, given a sentence $x = (x_1, x_2, x_3, ..., x_t)$ where a $x_i$ is the i-th word of the sentence, it will learn to predict the vector $y = (y_1, y_2, y_3, ...,y_t)$ where $y_t = x_{t+1}$. In other words it needs to predict at each time step $t$ the next word $x_{t+1}$. Now of course we need batches of those, so we get a matrix of sequences : \n",
    "\n",
    "$\n",
    "    x = \n",
    "    \\begin{bmatrix} \n",
    "        x_{11} & x_{12} & \\dots & x_{1bptt} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        x_{b1} &  x_{b2} & \\dots & x_{bbptt} \n",
    "    \\end{bmatrix}\n",
    "    ,  y = \n",
    "    \\begin{bmatrix} \n",
    "        x_{12} & x_{13} & \\dots & x_{1(bptt+1)} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        x_{b2} &  x_{b3} & \\dots & x_{b(bptt+1)} \n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where b is the batch size (bs) and m is the back propagation through time (bptt). We also need that for every batches, line $i$ of a batch must be the following of line $i$ form the last batch, i.e : Let $b_j$ and $b_{j+1}$ be two consecutive batches, then :\n",
    "\n",
    "if $(b_j)_i = (x_1, x_2, ..., x_bptt)$ then $(b_{j+1})_i = (x_{bptt+1}, x_{bptt+2},..., x_{2bptt})$. \n",
    "\n",
    "Now we have only have tweets, the idea is to concatenate all of them to have a full stream of tokens. So we initially have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$t = [ [t_{11}, t_{12},..., t_{1m_1}], [t_{21}, t_{22},..., t_{2m_2}], ... , [t_{T1}, t_{T2},..., t_{Tm_T}] ]$\n",
    "\n",
    "where $t$ is the lists of tweets, $t_{ij}$ is the jth token of the i'th tweet, $m_i$ is the size of the i'th tweet and $T$ is the total number of tweets. We then concanete to create the stream \n",
    "\n",
    "$stream = [t_{11}, t_{12},..., t_{1m_1}, t_{21}, t_{22},..., t_{2m_2}, ... , t_{T1}, t_{T2},..., t_{Tm_T}]$\n",
    "\n",
    "we can change notation to ease the explanations :\n",
    "\n",
    "$stream = [s_{1}, s_{2}, ...,  s_{M}]$\n",
    "\n",
    "we now split the text into $bs$ number of sequences to get the batched data:\n",
    "\n",
    "$\n",
    "    \\begin{bmatrix} \n",
    "        s_{1} & s_{2} & \\dots & s_{sl} \\\\\n",
    "        s_{sl+1} & s_{sl+2} & \\dots & s_{2sl} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        s_{((bs-1)sl) + 1} &  s_{((bs-1)sl) + 2} & \\dots & s_{(bs)(sl)} \n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "we then split this matrix vertically each $bptt$ tokens in orther to get our $n = \\frac{sl}{bptt} = \\frac{M}{bpttxbs}$ number of batches:\n",
    "\n",
    "$\n",
    "    \\begin{bmatrix} \n",
    "        s_{1}  & \\dots & s_{bptt} \\\\\n",
    "        s_{sl+1} & \\dots & s_{sl + bptt} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        s_{((bs-1)sl) + 1}  & \\dots & s_{((bs-1)sl) + bptt} \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "        s_{bptt + 1} & \\dots & s_{2bptt} \\\\\n",
    "        s_{sl + bptt + 1} & \\dots & s_{sl + 2bptt} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        s_{((bs-1)sl) + bptt + 1} & \\dots & s_{((bs-1)sl) + 2bptt} \n",
    "    \\end{bmatrix}\n",
    "    ...\n",
    "    \\begin{bmatrix} \n",
    "        s_{(n-1)bptt + 1} & \\dots & s_{nxbptt} \\\\\n",
    "        s_{sl + (n-1)bptt + 1} & \\dots & s_{sl + nxbptt} \\\\\n",
    "        \\vdots & \\ddots & \\\\\n",
    "        s_{((bs-1)sl) + (n-1)bptt + 1} & \\dots & s_{((bs-1)sl) + nxbptt} \n",
    "    \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target batches, it is the same, only the elements are shifted by one as explained above.\n",
    "\n",
    "The implementation is done using the **LM_Preloader** and the paytorch **DataLoader** class. Additionally we can shuffle the tweets before creating the stream in order to randomize the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_PreLoader():\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle\n",
    "        total_len = sum([len(t) for t in data.x])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
    "\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the training and validation language model DataLoaders \n",
    "    \"\"\"\n",
    "    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification, the data preperation is quite simpler. Each data tuple $(x,y)$ is simply a numericalized tweet $x$ and it's label $y$ (0 or 1). This gets a bit more tricky when we have to create batches of those. The tweets won't have the same lengths obviously hence we must use padding for the sake of same lengths tweets to use them in a batch.\n",
    "What we do is everytime we get a batch of tweets, we measure the length of the longest tweet and pad the rest to have the same length as the longest one.\n",
    "\n",
    "One situation we would not like to arrive at is for example having simultaneously long and short tweets in a batch, this would mean we would have to do a lot of padding. To address, we use a **Sampler** that we ill be given to the **DataLoader** which can basically change the order of the DataLoader iterator. We then use the **collate_fn** attribute of the **DataLoader** to pad the tweets in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SortSampler** implements what we just mentionned and will be used for the validation dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training dataloader we also want to have some randomization in the training order which is done using the **SortishSampler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        # We also want to have the boggest batch at the beggining and the smallest one at the end for memory reasons\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we use a class Databunch (similar to the fastai DataBunch) which will contain the train and valid dataloaders and the vocab. We then create two subclasses for the language model data and the classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Databunch() :\n",
    "    \"\"\"\n",
    "    Container of a Dataloaders for the validation and training datasets \n",
    "\n",
    "    Arguments:\n",
    "        train_dl: The training dataloader (must implement __iter__ method)\n",
    "        valid_dl: The validaation dataloader (must implement __iter__ method)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dl, valid_dl, vocab) :\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset  \n",
    "    \n",
    "    def save(self, path) :\n",
    "        pickle.dump((self.train_dl, self.valid_dl, self.vocab), open(path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDatabunch(Databunch) :\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, path, text_col, pctg, bs=64, bptt=70, vocab=None) :\n",
    "        tl = TextList.from_csv(path, text_col)\n",
    "        sd = SplitData.split_by_func(tl, partial(random_splitter, pctg=0.2))\n",
    "        proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor(vocab=vocab)\n",
    "        ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])\n",
    "        train_dl, valid_dl = get_lm_dls(ll.train, ll.valid, bs, bptt)\n",
    "        return cls(train_dl, valid_dl, proc_num.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasDatabunch(Databunch) :\n",
    "\n",
    "    def save(self, path) :\n",
    "        pickle.dump((self.train_dl.dataset, self.valid_dl.dataset, self.vocab, self.train_dl.batch_size), open(path, 'wb'))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, path, text_col, label_col, pctg, bs=64, vocab=None) :\n",
    "        df = pd.read_csv(path)\n",
    "        tl = TextList.from_df(df, text_col)\n",
    "        sd = SplitData.split_by_func(tl, partial(random_splitter, pctg=0.2))\n",
    "        tweet_to_label = {}\n",
    "        it = tqdm_notebook(range(df.shape[0]), total=df.shape[0])\n",
    "        for i in it : \n",
    "            tweet_to_label[df[text_col].iloc[i]] = df[label_col].iloc[i]\n",
    "        proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor(vocab=vocab)\n",
    "        ll = label_by_func(sd, lambda x: tweet_to_label[x], proc_x = [proc_tok,proc_num])\n",
    "        train_dl, valid_dl = get_clas_dls(ll.train, ll.valid, bs)\n",
    "        return cls(train_dl, valid_dl, proc_num.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, data_type=LMDatabunch) :\n",
    "    \"\"\"\n",
    "    Loads the databunch stored in path\n",
    "    \"\"\"\n",
    "    if data_type == LMDatabunch :\n",
    "        train_dl, valid_dl, vocab = pickle.load(open(path, 'rb'))\n",
    "        return data_type(train_dl, valid_dl, vocab)\n",
    "    else : \n",
    "        train_ds, valid_ds, vocab, bs = pickle.load(open(path, 'rb'))\n",
    "        train_dl, valid_dl = get_clas_dls(train_ds, valid_ds, bs)\n",
    "        return data_type(train_dl, valid_dl, bs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
