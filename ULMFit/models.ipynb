{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from fastai.datasets import untar_data \n",
    "import torch\n",
    "from fastai.text import get_language_model, convert_weights, get_text_classifier\n",
    "from torchtext.datasets import Multi30k, LanguageModelingDataset\n",
    "from models import *\n",
    "from torchtext.data import *\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import os\n",
    "import csv\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from fastai.core import even_mults\n",
    "from fastai.callback import annealing_cos, annealing_exp, annealing_linear\n",
    "from typing import Callable, Union\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the full implementation of the language model and the text classifier model used in the [ULMFIT](https://arxiv.org/pdf/1801.06146.pdf) paper. Most of the upcoming code is heavily based on the [fastai](https://docs.fast.ai) library and its deep learning course, which has already a full implementation of the ulmfit approach for NLP. However considering the complexity of the fastai code and its simplicity to use we figured it would helpful for readers to get a full bottom up implementation using pytorch as a baseline. Most of the code is exported to training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model : AWD_LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the AWD LSTM architecture is of course the LSTM neural net. It is an improvement to the standart RNN way of dealing with sequential data (such as text). LSTM deals with the [vanishing/exploding gradient problem](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) that come up in simple RNNs using cell connection gates. For further intuition on 'why' most the upcoming implentations I recommend colah's blog post : [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell and equations](images/lstm.jpg)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM archutecture is composed of a repeated cell which is shown in the image above. Its inputs are :\n",
    "- **xt** which in our case is the embedding vector of the nth word of a batch of sentences\n",
    "- **ht-1** the output of the last cell just like in RNNs.\n",
    "- **ct-1** again output form last cell which is called the *cell state* used to prevent long-term dependencies problem.\n",
    "\n",
    "The $\\sigma$ reprenstents the [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied element-wise to its input. Both x and + connections are elemnt-wise multiplication and addition respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement it using the pytorch nn.Module class. We use a two big matrix multiplication to compute x*U and and h*U instead of 4 for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.h_s = h_s\n",
    "        self.x_s = x_s\n",
    "        self.U = nn.Linear(x_s,4*h_s)\n",
    "        self.W = nn.Linear(h_s,4*h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        #inputs from last cell\n",
    "        h,c = state\n",
    "        \n",
    "        #computing itermedtiate gates\n",
    "        gates = (self.U(input) + self.W(h)).chunk(4, 1)\n",
    "        i_t,f_t,o_t = map(torch.sigmoid, gates[:3])\n",
    "        c_t = gates[3].tanh()\n",
    "        c = (f_t*c) + (i_t*c_t)\n",
    "        h = o_t * c.tanh()\n",
    "        \n",
    "        #outputting the usualt h output and the state to give to next cell if needed\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next building block of the LSTM is the LSTM layer wich consisit of appling the LSTM cell to each sequential input in a recurrent manner with each time forwarding its state to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM layer](images/LSTM3.png)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = LSTMCell(x_s, h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        # divide the input in the sequence dimension to get x_0, x_1, x_2, ...\n",
    "        inputs = input.unbind(1)\n",
    "        \n",
    "        #prepare to store the output of each cell\n",
    "        outputs = []\n",
    "        \n",
    "        #applying the cell recursively \n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.lstm_cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        \n",
    "        #return the stacked outputs\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the state for the first cell we simply use tensors with only zeroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before having fully implemented pytorch's LSTM module is stacking multiple LSTM layers one above each other as shown in the following diagram :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked RNN](images/RNN_Stacking.png)\n",
    "(picture from : https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/recurrent_neural_networks.html?q= )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='pink'>pink rectangles</font>  : The sequential inputs  \n",
    "- <font color='green'>green rectangles</font>  : An LSTM cell, each line is a layer so the cells on the same line are the same\n",
    "- <font color='blue'>blue rectangles</font> : The sequential outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous implementations, we created the initial state **h0** and **c0** outside of the model at the same time as we gave the input. This means that we could create h0 and c0 with the right sizes by simply comparing it to the input sizes. This time around, we will create the initial state to give to all the layers inside the model itself. As the dimensions of the state depend on the batch size of the input given, we need to create the initial states at the start of the forward pass when we are given the input (and thus we know the batch size) with the help of the **reset()** method. We also want to keep the last states from last batch if the batch size did not change.\n",
    "\n",
    "We aso have to be careful of the sives of the hidden layers inputs. For example, on the image above, the first layer takes as input the initial sequence and outputs a hidden sequence whereas the second and third layers take as input a hidden sequence and outputs a hidden sequence. Because of that we must have a different sizes for the first layer and the other layers. And of course the same goes for the initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullLSTM(nn.Module):\n",
    "    def __init__(self, x_s, h_s, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMLayer(x_s if i==0 else h_s, h_s ) for i in range(n_layers)])\n",
    "        self.bs = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        #get the batch size from the first dimension of the input \n",
    "        bs, sl, _ = input.size()\n",
    "        if self.bs != bs :\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "       \n",
    "        # now we have the initial states and we can go through all the layers recursively\n",
    "        for j in range(self.n_layers) :\n",
    "            layer = self.lstm_layers[j]\n",
    "            input, self.hidden[j] = layer(input, self.hidden[j])\n",
    "                \n",
    "        \n",
    "        #return the outputs\n",
    "        return input\n",
    "    \n",
    "    def reset(self) :\n",
    "        st = next(self.parameters()).new(self.bs, h_s).zero_()\n",
    "        self.hidden = [(st, st) for l in range(self.n_layers)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is pretty much the same as pytorch's nn.LSTM. The difference is that pytorch uses CuDNN to make the computations faster. We will now use pytorch's implementation instead of ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization : Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual regularization techniques used in feed-forward and convolutional neural nets such as dropout and batchnorm do not work well in RNNs. The AWD LSTM uses extensions of those to regularize its model. Correspondigns ections of the [paper](https://arxiv.org/pdf/1708.02182.pdf) will be provided for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational dropout \n",
    "Section 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in variational dropout is to use the **same** drop out mask to a squential input over the sequence dimension. In essence, if you have an input x with shape *(bs, seq_len, x_s)*, the dropout mask will be of shape *(bs, 1, x_s)* and will be applied to each slice of sequence.\n",
    "This dropout will be used on each output/input of the LSTM layers. Additionally we divide every activations that have not been set to 0 by the mask by 1-p (p: probability of dropout) to keep the average. We use [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) to be efficient in the element-wise computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDropout(nn.Module) :\n",
    "    def __init__(self, p=0.5) :\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x) :\n",
    "        #The dropout should only be used during training and not eval \n",
    "        if not self.training or self.p == 0.: return x\n",
    "        #the mask\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        #element-wise multiplication with broadcasting\n",
    "        return x*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.9241e+00,  7.5969e-01, -8.2366e-01,  1.6091e+00, -2.4311e-01,\n",
       "           -1.7052e-01,  3.5545e+00],\n",
       "          [ 2.9414e-03, -3.9516e-01, -1.1121e-01,  8.0972e-01,  3.7951e-01,\n",
       "            1.6787e+00,  1.2045e+00],\n",
       "          [-2.0720e+00,  4.5873e-02,  3.1458e-01, -6.5350e-02,  6.6786e-01,\n",
       "           -4.8988e-02,  5.2090e-01]],\n",
       " \n",
       "         [[ 1.2378e-01, -2.0185e+00, -2.4288e+00, -5.2116e-01,  2.1887e-01,\n",
       "           -3.1126e-01,  1.4472e+00],\n",
       "          [ 2.8520e+00,  6.5735e-01,  1.2059e+00,  1.4011e-02, -9.4021e-01,\n",
       "            3.2747e-02,  4.0664e-01],\n",
       "          [ 5.6582e-02,  1.4144e+00,  4.7610e-02, -6.4115e-01, -1.8566e-01,\n",
       "           -3.7903e-01, -3.3611e-01]],\n",
       " \n",
       "         [[-1.1319e+00, -4.4765e-01, -2.4535e-01,  7.7542e-01,  1.8033e+00,\n",
       "            1.8792e+00,  1.8971e+00],\n",
       "          [-1.0683e+00,  6.1346e-01,  5.0164e-01,  4.0671e-01,  5.9879e-01,\n",
       "            7.1216e-01,  1.3058e-01],\n",
       "          [-8.0786e-01,  1.6683e+00,  2.0653e-01,  5.0945e-01,  1.8499e+00,\n",
       "           -4.8998e-01,  1.1390e+00]]]),\n",
       " tensor([[[ 0.0000,  1.0853, -0.0000,  2.2988, -0.3473, -0.0000,  5.0778],\n",
       "          [ 0.0000, -0.5645, -0.0000,  1.1567,  0.5422,  0.0000,  1.7207],\n",
       "          [-0.0000,  0.0655,  0.0000, -0.0934,  0.9541, -0.0000,  0.7441]],\n",
       " \n",
       "         [[ 0.1768, -2.8835, -3.4697, -0.7445,  0.3127, -0.0000,  0.0000],\n",
       "          [ 4.0743,  0.9391,  1.7228,  0.0200, -1.3432,  0.0000,  0.0000],\n",
       "          [ 0.0808,  2.0206,  0.0680, -0.9159, -0.2652, -0.0000, -0.0000]],\n",
       " \n",
       "         [[-1.6170, -0.0000, -0.0000,  1.1077,  0.0000,  2.6845,  2.7101],\n",
       "          [-1.5261,  0.0000,  0.0000,  0.5810,  0.0000,  1.0174,  0.1865],\n",
       "          [-1.1541,  0.0000,  0.0000,  0.7278,  0.0000, -0.7000,  1.6272]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = VDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "tst_input, m(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dropped is consistent in the second dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding dropout \n",
    "Section 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding dropout we simply nulifiy entire rows of the word embedding matrix with probability p. Again broadcastiong is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mEmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
       "        [ 0.5269,  0.9033, -0.8586,  1.7035,  0.3431, -0.5421, -1.1035],\n",
       "        [-0.2606,  0.9515, -1.7171,  1.4127, -5.0057, -1.4582,  0.3003],\n",
       "        [ 2.6632, -1.5893, -1.7917, -0.9815, -0.1710,  2.1583,  4.5810],\n",
       "        [ 1.6075, -3.0090,  0.3957, -3.6681,  0.0316,  0.0674,  1.3095],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 3.1144, -3.0127,  0.9502,  1.9643, -1.2018,  0.4565, -0.9475],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = mEmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that entire rows have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight-dropout\n",
    "Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight dropout is a dropout applied to the weights inside the LSTM cells : U and W.\n",
    "\n",
    "In order to keep the speed of the LSTM layer, we simply replace the weight matrix of the LSTM by a masked version and keep the non-masked version. We can then simply apply the LSTM layer and it will use its new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the parameter in the nn.LSTM module containing the weights \n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class mWeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            #\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2535, -0.6309],\n",
       "        [ 0.7069, -0.1329],\n",
       "        [ 0.2839, -0.0401],\n",
       "        [ 0.0010,  0.2045],\n",
       "        [-0.0550, -0.2916],\n",
       "        [ 0.3217, -0.6281],\n",
       "        [ 0.4655, -0.6114],\n",
       "        [-0.6816,  0.1285]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = mWeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4225, -1.0515],\n",
       "        [ 1.1782, -0.2215],\n",
       "        [ 0.0000, -0.0000],\n",
       "        [ 0.0016,  0.3409],\n",
       "        [-0.0000, -0.0000],\n",
       "        [ 0.5362, -0.0000],\n",
       "        [ 0.7759, -0.0000],\n",
       "        [-0.0000,  0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4,20,5)\n",
    "h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n",
    "x,h = dp_module(tst_input,h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dropout is applied to the weights during the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have everything ready to implement the entire AWSD LSTM model, the following code might look really complicated at first but it is in fact pretty much the same as our fullLSTM except we use the different kinds of dropout disscussed above. It also takes care of the word embeddings whereas our fullLSTM assumed it was already done so we need to take care of that. Another difference is that the last layer outputs a different size tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mAWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"Returns an iterator over module parameters.\n",
    "\n",
    "        This is typically passed to an optimizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_sz (int): number of words in the vocab\n",
    "            emb_sz (int): size of the word embedding vector\n",
    "            n_hid (int): size of the hidden vector \n",
    "            n_layers (int): number of layers in the LSTM\n",
    "            pad_token (int): id of the pad_idx for the embedding matrix\n",
    "            hidden_p (float): dropout probability for variational dropout on hidden activations\n",
    "            input_p (float): dropout probability for variational dropout on input activations\n",
    "            embed_p (float):dropout probability for embedding dropout \n",
    "            weight_p (float):dropout probability for weight dropout\n",
    "\n",
    "        \"\"\"\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = mEmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([mWeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = VDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([VDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        print(raw_output.shape)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a decoder which takes the output of our AWD_LSTM and transform it into the prediction of the wrord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = VDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both of them using a sequential module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mSequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mget_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = mAWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, mLinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code from the last couple of cells is actually already implemented in the fastai library with minor changes with the same functions/class names without the m at the beggining. For example here is the model generated from our implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): mAWD_LSTM(\n",
       "    (emb): Embedding(400, 20, padding_idx=399)\n",
       "    (emb_dp): mEmbeddingDropout(\n",
       "      (emb): Embedding(400, 20, padding_idx=399)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): mWeightDropout(\n",
       "        (module): LSTM(20, 100, batch_first=True)\n",
       "      )\n",
       "      (1): mWeightDropout(\n",
       "        (module): LSTM(100, 100, batch_first=True)\n",
       "      )\n",
       "      (2): mWeightDropout(\n",
       "        (module): LSTM(100, 20, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): VDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): VDropout()\n",
       "      (1): VDropout()\n",
       "      (2): VDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): mLinearDecoder(\n",
       "    (output_dp): VDropout()\n",
       "    (decoder): Linear(in_features=20, out_features=400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mget_language_model(vocab_sz=400, emb_sz=20, n_hid=100, n_layers=3, pad_token=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the dame using fastai's get_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(400, 20, padding_idx=399)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(400, 20, padding_idx=399)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(20, 100, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(100, 100, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(100, 20, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=20, out_features=400, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {'emb_sz':20, 'n_hid':100, 'n_layers':3, 'pad_token':-1, 'output_p':0.4, 'hidden_p':0.2, 'input_p':0.6, \n",
    "                       'embed_p':0.1, 'weight_p':0.5, 'tie_weights':True, 'out_bias':True}\n",
    "lm = get_language_model(AWD_LSTM, 400, configs)\n",
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, they are both the same with only module names changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly load fastai's awd_lstm pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_lm(vocab) :    \n",
    "    \"\"\"\n",
    "    Load fastai's pretrained awd_lstm model\n",
    "    \"\"\"\n",
    "    lm = get_language_model(AWD_LSTM, len(vocab))\n",
    "    model_path = untar_data('https://s3.amazonaws.com/fast-ai-modelzoo/wt103-1', data=False)\n",
    "    fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "    old_itos = pickle.load(open(fnames[1], 'rb'))\n",
    "    old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "    wgts = torch.load(fnames[0], map_location=lambda storage, loc: storage)\n",
    "    wgts = convert_weights(wgts, old_stoi, vocab)\n",
    "    lm.load_state_dict(wgts)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier model will have the same core as the language model which is the AWD_LSTM model with some minor modifications to ignore the padding. Will will then have on top of that some concat pooling of the results of the core which will then have a linear classifier on top. ([ulmfit paper](https://arxiv.org/pdf/1801.06146.pdf) section 3 for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified AWD_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, our RNN inputs will have some padding that we want to ignore when giving it to the layers. In order to do that while keeping the CuDN speed of our LSTM layers we use pytorch **pack_padded_sequence** and **pad_packed_sequence** ([docs](https://pytorch.org/docs/stable/nn.html)) which deals with padded tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM_clas(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        \n",
    "        ##new\n",
    "        mask = (input == self.pad_token)\n",
    "        lengths = sl - mask.long().sum(1)\n",
    "        n_empty = (lengths == 0).sum()\n",
    "        if n_empty > 0:\n",
    "            input = input[:-n_empty]\n",
    "            lengths = lengths[:-n_empty]\n",
    "            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]\n",
    "            \n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True) #new\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0] #new\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "            new_hidden.append(new_h)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs, mask\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last element/average/maximum. (again refer to paper for more info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        return output,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the head of our model which is a concat pooling with a classifer. We create a **PoolingLinearClassifier** which can have multiple layers and uses dropout and batchnorm on the activations (and of course ReLU for non-linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to take care of the fact that our tweets might be too long to givto one LSTM layer, so we need to divide the batches into smaller batches of shape $(bs, bptt)$ which will be done using a **SentenceEncoder** over our awd_lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(t, bs, val=0.):\n",
    "    if t.size(0) < bs:\n",
    "        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, module, bptt, pad_idx=1):\n",
    "        super().__init__()\n",
    "        self.bptt,self.module,self.pad_idx = bptt,module,pad_idx\n",
    "\n",
    "    def concat(self, arrs, bs):\n",
    "        return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.module.bs = bs\n",
    "        self.module.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            masks.append(pad_tensor(m, bs, 1))\n",
    "            raw_outputs.append(r)\n",
    "            outputs.append(o)\n",
    "        return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mget_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, \n",
    "                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = AWD_LSTM_clas(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = SentenceEncoder(rnn_enc, bptt)\n",
    "    if layers is None: layers = [50]\n",
    "    if drops is None:  drops = [0.1] * len(layers)\n",
    "    layers = [3 * emb_sz] + layers + [n_out] \n",
    "    drops = [output_p] + drops\n",
    "    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt=70\n",
    "emb_sz, nh, nl = 300, 300, 2\n",
    "dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n",
    "model = mget_text_classifier(60000, emb_sz, nh, nl, 2, 1, bptt, *dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM_clas(\n",
       "      (emb): Embedding(60000, 300, padding_idx=1)\n",
       "      (emb_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 300, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(300, 300, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(300, 300, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.10000000149011612)\n",
       "      (2): Linear(in_features=900, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the language model, most of the code is already in the fastai library with some minor changes, so we can directly use get_text_classifier from fastai :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n",
    "                       hidden_p=0.3, input_p=0.4, embed_p=0.05, weight_p=0.5)\n",
    "model = get_text_classifier(AWD_LSTM, 60000, 2, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.4)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBatchEncoder(\n",
       "  (module): AWD_LSTM(\n",
       "    (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again need to get the parameters of the model in group to apply discriminative learning rate and gradual unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_model_param_groups(classifier_model) :\n",
    "    \"\"\"\n",
    "    Returns the parameter groups structured by the RNN layers of the classifier model\n",
    "    \"\"\"\n",
    "    parameters = []\n",
    "    parameters.append({'params' : chain(classifier_model[0].module.encoder.parameters(), classifier_model[0].module.encoder_dp.parameters())})\n",
    "    for rnn in classifier_model[0].module.rnns :\n",
    "        parameters.append({'params' : rnn.parameters()})\n",
    "    parameters.append({'params' : classifier_model[1].parameters()})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leanrner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code simplicity, we refactor all the training data in the Learner class (similar to fastai's Learner).\n",
    "The learner class contains the model, the optimizer, the loss function and the data. It can also freeze layers (make them not train) for gradual unfreezing. We create two subclasses for the classifer and the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    \"\"\"\n",
    "    Container for a deep learning model, an optimizer, a loss function and the data containing the training and validation \n",
    "    with possibility to train the model\n",
    "\n",
    "    Arguments :\n",
    "        model (nn.Module): the pytorch model\n",
    "        opt (torch.optim): the pytorch optimizer\n",
    "        loss_func (Callable): the loss function \n",
    "        data (Databunch): the data containing the training and validation dataloaders\n",
    "    \"\"\"\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
    "    \n",
    "    def freeze_to(self, n) :\n",
    "        \"\"\"\n",
    "        Freezes the optimizer parameter group up to n\n",
    "        \"\"\"\n",
    "        if n >= len(self.opt.param_groups) :\n",
    "            raise ValueError(f'The optimizer only has {len(self.opt.param_groups)} parameter groups')\n",
    "        \n",
    "        for g in self.opt.param_groups[:n]:\n",
    "            for l in g['params']:\n",
    "                l.requires_grad=False\n",
    "        for g in self.opt.param_groups[n:]: \n",
    "            for l in g['params']:\n",
    "                l.requires_grad=True\n",
    "    \n",
    "    def unfreeze(self) :\n",
    "        \"\"\"\n",
    "        Unfreezes the whole parameter groups \n",
    "        \"\"\"\n",
    "        self.freeze_to(0)\n",
    "\n",
    "    def save(self, path) :\n",
    "        state = {'model' : self.model.state_dict(), 'opt' : self.opt.state_dict()}\n",
    "        torch.save(state, path)\n",
    "    \n",
    "    def load(self, path) :\n",
    "        state = torch.load(path)\n",
    "        self.model.load_state_dict(state['model'])\n",
    "        self.opt.load_state_dict(state['opt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLanguageLearner(Learner) :\n",
    "    \n",
    "    def save_encoder(self, path) :\n",
    "        \"\"\"\n",
    "        saves the encoder part of the model, which will then be loaded by the classifer\n",
    "        \"\"\"\n",
    "        torch.save(self.model[0].state_dict(), path)\n",
    "    \n",
    "    def fit(self, epochs, **kwargs) :\n",
    "        return fit(epochs, self, lm=True, **kwargs)\n",
    "\n",
    "    def validate(self, cuda=True) :\n",
    "        validate(self, cuda, lm=True)\n",
    "    \n",
    "class TextClassifierLearner(Learner) :\n",
    "    \n",
    "    def fit(self, epochs, **kwargs) :\n",
    "        return fit(epochs, self, lm=False, **kwargs)\n",
    "    \n",
    "    def validate(self, cuda=True) :\n",
    "        validate(self, cuda, lm=False)\n",
    "\n",
    "    def add_test(self, test_dl) :\n",
    "        self.test_dl = test_dl\n",
    "    \n",
    "    def predict_test(self) :\n",
    "        if self.test_dl is None :\n",
    "            return \n",
    "        preds = []\n",
    "        self.model = self.model.cuda()\n",
    "        batches = tqdm_notebook(self.test_dl, leave=False,\n",
    "                    total=len(self.test_dl), desc=f'Predictions')\n",
    "        for x, _ in batches :\n",
    "            x = x.cuda()\n",
    "            pred = self.model(x)[0]\n",
    "            preds.append(torch.argmax(pred, dim=1))\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        return preds\n",
    "    \n",
    "    def make_submission(self, path) :\n",
    "        preds = self.predict_test()\n",
    "        preds = (preds - (preds == 0).type(torch.cuda.LongTensor)).tolist()\n",
    "        sub = pd.DataFrame({'Id' : range(1, len(preds)+1), 'Prediction' : list(preds)})\n",
    "        sub.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_learner(data, opt_func=torch.optim.Adam, loss_func=CrossEntropyFlat(), lr=0.01) :\n",
    "    model = load_pretrained_lm(data.vocab)\n",
    "    opt = opt_func(get_lang_model_param_groups(model), lr=lr)\n",
    "    return TextLanguageLearner(model, opt, loss_func, data)\n",
    "\n",
    "def get_classifier_learner(data, enc_path, opt_func=torch.optim.Adam, loss_func=CrossEntropyFlat(), lr=0.01) :\n",
    "    model = get_text_classifier(AWD_LSTM, len(data.vocab), 2)\n",
    "    load_encoder_clas(model, enc_path)\n",
    "    opt = opt_func(get_class_model_param_groups(model), lr=lr)\n",
    "    return TextClassifierLearner(model, opt, loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop implement th following (taken from [ULMFIT](https://arxiv.org/pdf/1801.06146.pdf) and [A disciplined approach to neural network hyper parameters part 1](https://arxiv.org/pdf/1803.09820.pdf)) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradiant clipping\n",
    "- Activation regularization (AR)\n",
    "- Temporal activation regularization (TAR)\n",
    "- discriminative learning rate \n",
    "- cyclical learning rates and mometnum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn, lm, cuda=True, show_info=True, grad_clip=0.1, alpha=2., beta=1., record=True, one_cycle=True, \n",
    "                 max_lr:Union[float,slice]=0.01,  div_factor:float=25., pct_start:float=0.3, final_div:float=None, moms=(0.95, 0.85),\n",
    "                 annealing:Callable=annealing_cos, notebook=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the learner for a number of epochs\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "        epochs : number of epochs\n",
    "        learn : the Learner\n",
    "        cuda : if we train on gpu or not\n",
    "        show_info : show training and validation loss and accuracy\n",
    "        grad_clip : use fro gradiant clipping \n",
    "        alpha :  activation regularization parameter \n",
    "        beta : temporal activation regularization parameter\n",
    "        record : to record hyperparameters (learning rate and mometnum) and losses\n",
    "        one_cycle : for using cycling learning rate and momentum\n",
    "        max_lr : the max learning rate for the cycle (if max_lr is a slice then discriminative learning rate is applied)\n",
    "        div_factor : factor to divide max_lr to get the starting learning rate for the cycle \n",
    "        pct_start : at which fraction of the cycle do we reach max_lr\n",
    "        final_div : factor to divide max_lr to get the ending learning rate for the cycle\n",
    "        moms : the maximum and lowest momentum for the cycle \n",
    "        annealing : the interpolation function for the learning rate and the momentum \n",
    "    \"\"\"\n",
    "     #number of batches in one epoch for validation and training data\n",
    "    train_size = len(learn.data.train_dl)\n",
    "    valid_size = len(learn.data.valid_dl)\n",
    "    \n",
    "    # total iterations and cut used for slanted_triangular learning rates (T and cut from paper)\n",
    "    total_iterations = epochs*train_size\n",
    "\n",
    "    if record:\n",
    "        momentum = [[] for i in range(len(learn.opt.param_groups))]\n",
    "        lrs_record = [[] for i in range(len(learn.opt.param_groups))]\n",
    "        train_losses = []\n",
    "        val_losses =[]\n",
    "        train_accs = []\n",
    "        valid_accs =[]\n",
    "    \n",
    "    #puts model on gpu\n",
    "    if cuda :\n",
    "        learn.model.cuda()\n",
    "       \n",
    "    #Start the epoch\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if hasattr(learn.data.train_dl.dataset, \"batchify\"): learn.data.train_dl.dataset.batchify()\n",
    "\n",
    "        #loss and accuracy \n",
    "        train_loss, valid_loss, train_acc, valid_acc = 0, 0, 0, 0\n",
    "\n",
    "        #puts the model on training mode (activates dropout)\n",
    "        learn.model.train()\n",
    "        \n",
    "        if notebook :\n",
    "            batches_train = tqdm_notebook(learn.data.train_dl, leave=False,\n",
    "                    total=len(learn.data.train_dl), desc=f'Epoch {epoch} training')\n",
    "        else :\n",
    "            batches_train = tqdm(learn.data.train_dl, leave=False,\n",
    "                    total=len(learn.data.train_dl), desc=f'Epoch {epoch} training')\n",
    "\n",
    "        \n",
    "        #batch number counter\n",
    "        batch_num = 0\n",
    "\n",
    "        learn.model.reset()\n",
    "       \n",
    "        #starts sgd for each batches\n",
    "        for x, y in batches_train:\n",
    "            \n",
    "            #cyclical learning rate and momentum\n",
    "            if one_cycle :\n",
    "                \n",
    "                cut = int(total_iterations*pct_start)\n",
    "                iteration = (epoch * train_size) + batch_num\n",
    "                \n",
    "                #next we compute the maximum lrs for each layer of our model, we can use either discriminative\n",
    "                #learning rate or the same learning rate for each layer\n",
    "                \n",
    "                #if we use discriminative learning rates\n",
    "                if isinstance(max_lr, slice) :\n",
    "                    max_lrs = even_mults(max_lr.start, max_lr.stop, len(learn.opt.param_groups))\n",
    "                \n",
    "                #else we give the same max_lr to every layer of the model\n",
    "                else :\n",
    "                    max_lrs = [max_lr for i in range(len(learn.opt.param_groups))]\n",
    "                \n",
    "                #the final learning rate division factor\n",
    "                if final_div is None: final_div = div_factor*1e4\n",
    "                \n",
    "                #computting the learning rate and momentum \n",
    "                if iteration < cut :\n",
    "                    lrs = [annealing(lr/div_factor, lr, iteration/cut) for lr in max_lrs]\n",
    "                    mom = annealing(moms[0], moms[1], iteration/cut) \n",
    "                else :\n",
    "                    lrs = [annealing(lr, lr/final_div, (iteration-cut)/(total_iterations-cut)) for lr in max_lrs]\n",
    "                    mom = annealing(moms[1], moms[0], (iteration-cut)/(total_iterations-cut))\n",
    "                \n",
    "                for i, param_group, lr in zip(range(len(learn.opt.param_groups)), learn.opt.param_groups, lrs) :\n",
    "                    param_group['lr'] = lr\n",
    "                    param_group['betas'] = (mom ,param_group['betas'][1])\n",
    "                    if record :\n",
    "                        lrs_record[i].append(lr)\n",
    "                        momentum[i].append(mom)\n",
    "            \n",
    "            batch_num+=1\n",
    "\n",
    "           #forward pass\n",
    "            if cuda :\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            pred, raw_out, out = learn.model(x)\n",
    "            loss = learn.loss_func(pred, y)\n",
    "            \n",
    "            #activation regularization \n",
    "            if alpha != 0.:  loss += alpha * out[-1].float().pow(2).mean()\n",
    "            \n",
    "            #temporal activation regularization \n",
    "            if beta != 0.:\n",
    "                h = raw_out[-1]\n",
    "                if len(h)>1: loss += beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "            \n",
    "            train_loss += loss\n",
    "            if lm :\n",
    "                train_acc += (torch.argmax(pred, dim=2) == y).type(torch.FloatTensor).mean() \n",
    "            else :\n",
    "                train_acc += (torch.argmax(pred, dim=1) == y).type(torch.FloatTensor).mean() \n",
    "\n",
    "            # compute gradients and updtape parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            #gradient clipping\n",
    "            if grad_clip:  nn.utils.clip_grad_norm_(learn.model.parameters(), grad_clip)\n",
    "            \n",
    "            #optimizationm step\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        train_loss = train_loss/train_size\n",
    "        train_acc = train_acc/train_size\n",
    "        \n",
    "\n",
    "        # putting the model in eval mode so that dropout is not applied\n",
    "        learn.model.eval()\n",
    "\n",
    "        if notebook :\n",
    "            batches_valid = tqdm_notebook(learn.data.valid_dl, leave=False,\n",
    "                total=len(learn.data.valid_dl), desc=f'Epoch {epoch} validation')\n",
    "        else : \n",
    "            batches_valid = tqdm(learn.data.valid_dl, leave=False,\n",
    "                total=len(learn.data.valid_dl), desc=f'Epoch {epoch} validation')    \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for x, y in batches_valid: \n",
    "                if cuda :\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                pred = learn.model(x)[0]\n",
    "                loss = learn.loss_func(pred, y)\n",
    "\n",
    "                valid_loss += loss\n",
    "                if lm :\n",
    "                    valid_acc += (torch.argmax(pred, dim=2) == y).type(torch.FloatTensor).mean() \n",
    "                else :\n",
    "                    valid_acc += (torch.argmax(pred, dim=1) == y).type(torch.FloatTensor).mean() \n",
    "                \n",
    "        valid_loss = valid_loss/valid_size\n",
    "        valid_acc = valid_acc/valid_size\n",
    "        \n",
    "        if show_info :\n",
    "            print(\"Epoch {:.0f} training loss : {:.3f}, train accuracy : {:.3f}, validation loss : {:.3f}, valid accuracy : {:.3f}\".format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "        if record :\n",
    "            val_losses.append(valid_loss)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_accs.append(valid_acc)\n",
    "    \n",
    "    if record :\n",
    "        return {'train_loss' : train_losses, 'valid_loss' : val_losses, 'train_acc': train_acc, 'valid_acc' : valid_acc, 'lrs' : lrs_record, 'momentums' : momentum}    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a function to just test the Learner on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(learn, cuda=True, lm=True) :\n",
    "    \"\"\"\n",
    "    Computes the validation loss and accuracy of the learner\n",
    "    \"\"\"\n",
    "    valid_size = len(learn.data.valid_dl)\n",
    "    \n",
    "    #puts model on gpu\n",
    "    if cuda :\n",
    "        learn.model.cuda()\n",
    "    else :\n",
    "        learn.model.cpu()\n",
    "    \n",
    "    #loss and accuracy \n",
    "    valid_loss, valid_acc = 0, 0\n",
    "\n",
    "\n",
    "    # putting the model in eval mode so that dropout is not applied\n",
    "    learn.model.eval()\n",
    "    with torch.no_grad():\n",
    "        batches = tqdm_notebook(learn.data.valid_dl, leave=False,\n",
    "                total=len(learn.data.valid_dl), desc=f'Validation')\n",
    "        for x, y in batches: \n",
    "            if cuda :\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            pred = learn.model(x)[0]\n",
    "            loss = learn.loss_func(pred, y)\n",
    "\n",
    "            valid_loss += loss\n",
    "            if lm :\n",
    "                valid_acc += (torch.argmax(pred, dim=2) == y).type(torch.FloatTensor).mean() \n",
    "            else :\n",
    "                valid_acc += (torch.argmax(pred, dim=1) == y).type(torch.FloatTensor).mean() \n",
    "                \n",
    "    valid_loss = valid_loss/valid_size\n",
    "    valid_acc = valid_acc/valid_size\n",
    "        \n",
    "    print(\"Loss : {:.3f}, Accuracy : {:.3f}\".format(valid_loss, valid_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering language model encoder to classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know need to transfer what we have learned in the language model to the classifier model, to to this we simply transfer the AWD_LSTM parameters which contain the embeddings and the LSTM layers. We already have a method in the LMTextLearner to save the encoder, now we ned one to loead it in the classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder_clas(model, enc_path):\n",
    "        model[0].module.load_state_dict(torch.load(enc_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
